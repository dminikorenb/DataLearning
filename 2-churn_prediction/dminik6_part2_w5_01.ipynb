{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# п. 4 - обучение на данных, которые есть в тестовом наборе"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import math\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import RidgeClassifier, LogisticRegression, SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.model_selection import cross_validate, learning_curve, StratifiedKFold\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4\\. Также вы уже реализовали несколько стратегий для обработки категориальных признаков. Сравните эти стратегии между собой с помощью оценки качества моделей по кросс-валидации, построенных на датасетах с использованием различных стратегий. Как обработка категориальных признаков сказывается на качестве модели? Какой вариант выглядит наиболее оптимальным с точки зрения качества?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate_rezult(cv_rez):\n",
    "    return [\n",
    "#         type(cv_rez['estimator'][0]).__name__, \n",
    "        type(cv_rez['estimator'][0][2]).__name__,\n",
    "        cv_rez['train_score'].mean(),\n",
    "        cv_rez['test_score'].mean(),\n",
    "        len(cv_rez['estimator']),\n",
    "        cv_rez['fit_time'].mean()            \n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = pd.read_csv('orange_small_churn_train_data.csv', index_col='ID')\n",
    "df_data.rename(columns={\"labels\": \"Label\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ограничим слова в переменных, которые будем использовать в тренировочном наборе\n",
    "# теми, что есть в тестовом наборе. Остальные заменим на other\n",
    "df_enable_words = pd.read_csv('orange_small_churn_test_data.csv', index_col='ID')\n",
    "for col in df_enable_words.select_dtypes(['object']).columns:\n",
    "    tmpset = set(df_enable_words[col].unique())\n",
    "    df_data[col] = df_data[col].apply(lambda x: x if x in tmpset else 'other')\n",
    "\n",
    "df_enable_words = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 18298 entries, 0 to 18297\n",
      "Columns: 213 entries, Var1 to Label\n",
      "dtypes: float64(174), int64(1), object(38)\n",
      "memory usage: 29.9+ MB\n"
     ]
    }
   ],
   "source": [
    "# Удаление столбцов, в которых все NaN. Кроме того, в Label есть NaN.\n",
    "# отбрасываем и эти строки.\n",
    "df_data = df_data[np.isfinite(df_data['Label'])].dropna(axis=1, how='all')\n",
    "# Удаление строк, в которых меньше 2х не NaN переменных (на этом наборе бесполезна)\n",
    "df_data.dropna(axis=0, how='all', inplace=True, thresh=2)\n",
    "df_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 18298 entries, 0 to 18297\n",
      "Columns: 213 entries, Var1 to Label\n",
      "dtypes: float64(174), int64(1), object(38)\n",
      "memory usage: 29.9+ MB\n"
     ]
    }
   ],
   "source": [
    "df_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_data = pd.read_csv('not_hold_out_dataset.csv', index_col=0)\n",
    "cat_var = df_data.select_dtypes(['object']).columns\n",
    "cat_num = df_data.select_dtypes(exclude=['object']).columns[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CalcCorr(Y, X):\n",
    "    iY0 = np.nonzero(Y==-1)\n",
    "    iY1 = np.nonzero(Y==1)\n",
    "    iX0 = np.nonzero(X==0)\n",
    "    iX1 = np.nonzero(X==1)\n",
    "#     print(iY0, iY1, iX0, iX1)\n",
    "    a = 1.*len(np.intersect1d(iY0, iX0))\n",
    "    b = 1.*len(np.intersect1d(iY0, iX1))\n",
    "    c = 1.*len(np.intersect1d(iY1, iX0))\n",
    "    d = 1.*len(np.intersect1d(iY1, iX1))\n",
    "    t = (a+b)*(c+d)*(a+c)*(b+d)\n",
    "    if t == 0.:\n",
    "        ret = 0\n",
    "        print()\n",
    "#         print(iY0, iY1, iX0, iX1)\n",
    "        print('a={}, b={}, c={}, d={}'.format(a, b, c, d))\n",
    "    else: \n",
    "        ret = (a*d-b*c)/(math.sqrt((a+b)*(c+d)*(a+c)*(b+d) ))\n",
    "    return ret\n",
    "\n",
    "# Y = np.array([-1, -1, -1, 1, 1, 1, 1], dtype='int8')\n",
    "# X = np.array([ 0,  0,  0, 0, 1, 0, 0], dtype='int8')\n",
    "# CalcCorr(Y, X)   \n",
    "# np.nonzero(Y==-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Var191 1 Var192 265 Var193 40 Var194 3 Var195 17 Var196 4 Var197 177 Var198 1460 Var199 906 Var200 1401 Var201 2 Var202 3224 Var203 3 Var204 100 Var205 3 Var206 21 Var207 12 Var208 2 Var210 6 Var211 2 Var212 63 Var213 1 Var214 1401 Var215 1 Var216 640 Var217 2891 Var218 2 Var219 15 Var220 1460 Var221 7 Var222 1460 Var223 4 Var224 1 Var225 3 Var226 23 Var227 7 Var228 26 Var229 4 Wall time: 50.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Методика отбора признаков - по наибольшей корреляции\n",
    "\n",
    "# Вычисление корреляции каждой переменной в каждом категориальном параметре\n",
    "# Цель - в дальнейшем оставить переменные и значения с наибольшей корреляцией\n",
    "# или коррелирующиеся.\n",
    "# Так как расчет выполняется несколько минут (только что - 7 минут, с введением фильтра \n",
    "# по количеству переменных равных 50 - 12 сек.),\n",
    "# то данные сохранены в файл\n",
    "acorr = []\n",
    "aLabels = np.array(df_data['Label'].values)\n",
    "# cat_var=['Var200']\n",
    "for c in cat_var:\n",
    "    \n",
    "    dVals = df_data[c].value_counts().to_dict()\n",
    "    df_dummies=pd.get_dummies(df_data[c])\n",
    "    print(c, len(dVals), end=' ')\n",
    "    d = np.array(df_dummies.values)\n",
    "    for i, c_val in enumerate(df_dummies.columns):\n",
    "#         if dVals[c_val] > 50:\n",
    "        rez = CalcCorr(aLabels, d[:,i])\n",
    "#             rez = CalcCorr(aLabels, np.array(df_dummies[c_val].values))\n",
    "        acorr.append([c, c_val, dVals[c_val], rez])\n",
    "\n",
    "            # df_data['Var193']\n",
    "df_corr_categ = pd.DataFrame(acorr, columns=['Var', 'Val', 'Count', 'correlation'], index=None)\n",
    "df_corr_categ.to_pickle('df_corr_categ_kaggle_01.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corr_categ = pd.read_pickle('df_corr_categ_kaggle_01.bin')\n",
    "df_corr_categ['correlation_abs'] = df_corr_categ['correlation'].abs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Var</th>\n",
       "      <th>Val</th>\n",
       "      <th>Count</th>\n",
       "      <th>correlation</th>\n",
       "      <th>correlation_abs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Var, Val, Count, correlation, correlation_abs]\n",
       "Index: []"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['Val'] == 'other']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(717, 5)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df_corr_categ[df_corr_categ['Val'] != 'other'].sort_values(by=['correlation_abs'], ascending =False)\n",
    "df = df[(df['Count'] > 3) & (df['correlation_abs'] > 0.012)]\n",
    "\n",
    "# Строка ниже подобрана для 'not_hold_out_dataset.csv'\n",
    "# df = df[(df['Count'] > 0) & (df['correlation_abs'] > 0.0187)]\n",
    "# При df[df['correlation_abs'] > 0.0188] плучено значение  roc-auc = 0.7491\n",
    "# с классификатором GradientBoostingClassifier\n",
    "# df = df[df['correlation_abs'] > 0.018]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 233 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_cat_var = df_data[df['Var'].unique()].copy()\n",
    "for col in df_cat_var.columns:\n",
    "    val_set = set(df[ df['Var'] == col]['Val'].values)\n",
    "    df_cat_var[col] = df_cat_var[col].apply(lambda x: x if x in val_set else 'other')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NhsEn4L          10687\n",
       "other             2660\n",
       "XfqtO3UdzaXh_     2359\n",
       "CrNX              1127\n",
       "Ie_5MZs            527\n",
       "4kVnq_T26xq1p      377\n",
       "_5OXC8MSLt         262\n",
       "h0lfDKh52u4GP      260\n",
       "TW8UhnX             39\n",
       "Name: Var212, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cat_var['Var212'].value_counts()\n",
    "# df_data['Var202'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_cat_var_dummies = pd.get_dummies(df_cat_var)\n",
    "# df_cat_var_dummies.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.concat([df_data[cat_num], df_cat_var_dummies ], axis=1)\n",
    "X = df_test.values\n",
    "# Массив с метками.\n",
    "y = df_data['Label'].values\n",
    "# df_test = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18298, 922)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "748"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(cat_num)  \n",
    "len(df_cat_var_dummies.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_cat_var_dummies = None\n",
    "# df_cat_var = None\n",
    "# df_data = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression 0.7554\n",
      "Wall time: 40.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_score, test_score = [], []\n",
    "\n",
    "cv = StratifiedKFold(n_splits=10, shuffle=True)   \n",
    "si = SimpleImputer(strategy='median')\n",
    "# 'median', 'most_frequent' mean\n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "# scaler = preprocessing.StandardScaler()\n",
    "rez = []\n",
    "for est in [\n",
    "#     SGDClassifier(), \n",
    "#     LinearSVC(),\n",
    "    LogisticRegression( solver='liblinear' ),\n",
    "#     RidgeClassifier(),\n",
    "#     RandomForestClassifier(), \n",
    "#     SVC(kernel='linear'),\n",
    "#     GradientBoostingClassifier()\n",
    "]:\n",
    "    pipe = Pipeline([\n",
    "        ('inputer', si), \n",
    "        ('scaler', scaler), \n",
    "        ('est', est)\n",
    "    ])\n",
    "    print(type(est).__name__, end=' ')\n",
    "    cv_rez = cross_validate(pipe, X, y, cv=cv, \n",
    "                          return_train_score=True, \n",
    "                          return_estimator=True,\n",
    "    \n",
    "                            verbose=0, n_jobs=4,\n",
    "                          scoring=('roc_auc'))\n",
    "    print(cv_rez['test_score'].mean().round(4))\n",
    "    rez.append(cross_validate_rezult(cv_rez))\n",
    "#     train_score.append(cv_rez['train_score'])\n",
    "#     test_score.append(cv_rez['test_score'])\n",
    "#     print(cv_rez['test_score'].mean().round(4), cv_rez['train_score'].mean().round(4), type(est).__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([10.27931046,  9.86524796, 10.31057787,  9.258816  ,  9.68277955,\n",
       "        12.92658329, 11.91898823, 11.19873929,  7.35449195,  6.36621094]),\n",
       " 'score_time': array([0.09474993, 0.08812022, 0.13833666, 0.10612583, 0.15217566,\n",
       "        0.09570289, 0.09472632, 0.11328125, 0.06250024, 0.046875  ]),\n",
       " 'estimator': (Pipeline(memory=None,\n",
       "           steps=[('inputer',\n",
       "                   SimpleImputer(add_indicator=False, copy=True, fill_value=None,\n",
       "                                 missing_values=nan, strategy='median',\n",
       "                                 verbose=0)),\n",
       "                  ('scaler', MinMaxScaler(copy=True, feature_range=(0, 1))),\n",
       "                  ('est',\n",
       "                   LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
       "                                      fit_intercept=True, intercept_scaling=1,\n",
       "                                      l1_ratio=None, max_iter=100,\n",
       "                                      multi_class='warn', n_jobs=None,\n",
       "                                      penalty='l2', random_state=None,\n",
       "                                      solver='liblinear', tol=0.0001, verbose=0,\n",
       "                                      warm_start=False))],\n",
       "           verbose=False), Pipeline(memory=None,\n",
       "           steps=[('inputer',\n",
       "                   SimpleImputer(add_indicator=False, copy=True, fill_value=None,\n",
       "                                 missing_values=nan, strategy='median',\n",
       "                                 verbose=0)),\n",
       "                  ('scaler', MinMaxScaler(copy=True, feature_range=(0, 1))),\n",
       "                  ('est',\n",
       "                   LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
       "                                      fit_intercept=True, intercept_scaling=1,\n",
       "                                      l1_ratio=None, max_iter=100,\n",
       "                                      multi_class='warn', n_jobs=None,\n",
       "                                      penalty='l2', random_state=None,\n",
       "                                      solver='liblinear', tol=0.0001, verbose=0,\n",
       "                                      warm_start=False))],\n",
       "           verbose=False), Pipeline(memory=None,\n",
       "           steps=[('inputer',\n",
       "                   SimpleImputer(add_indicator=False, copy=True, fill_value=None,\n",
       "                                 missing_values=nan, strategy='median',\n",
       "                                 verbose=0)),\n",
       "                  ('scaler', MinMaxScaler(copy=True, feature_range=(0, 1))),\n",
       "                  ('est',\n",
       "                   LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
       "                                      fit_intercept=True, intercept_scaling=1,\n",
       "                                      l1_ratio=None, max_iter=100,\n",
       "                                      multi_class='warn', n_jobs=None,\n",
       "                                      penalty='l2', random_state=None,\n",
       "                                      solver='liblinear', tol=0.0001, verbose=0,\n",
       "                                      warm_start=False))],\n",
       "           verbose=False), Pipeline(memory=None,\n",
       "           steps=[('inputer',\n",
       "                   SimpleImputer(add_indicator=False, copy=True, fill_value=None,\n",
       "                                 missing_values=nan, strategy='median',\n",
       "                                 verbose=0)),\n",
       "                  ('scaler', MinMaxScaler(copy=True, feature_range=(0, 1))),\n",
       "                  ('est',\n",
       "                   LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
       "                                      fit_intercept=True, intercept_scaling=1,\n",
       "                                      l1_ratio=None, max_iter=100,\n",
       "                                      multi_class='warn', n_jobs=None,\n",
       "                                      penalty='l2', random_state=None,\n",
       "                                      solver='liblinear', tol=0.0001, verbose=0,\n",
       "                                      warm_start=False))],\n",
       "           verbose=False), Pipeline(memory=None,\n",
       "           steps=[('inputer',\n",
       "                   SimpleImputer(add_indicator=False, copy=True, fill_value=None,\n",
       "                                 missing_values=nan, strategy='median',\n",
       "                                 verbose=0)),\n",
       "                  ('scaler', MinMaxScaler(copy=True, feature_range=(0, 1))),\n",
       "                  ('est',\n",
       "                   LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
       "                                      fit_intercept=True, intercept_scaling=1,\n",
       "                                      l1_ratio=None, max_iter=100,\n",
       "                                      multi_class='warn', n_jobs=None,\n",
       "                                      penalty='l2', random_state=None,\n",
       "                                      solver='liblinear', tol=0.0001, verbose=0,\n",
       "                                      warm_start=False))],\n",
       "           verbose=False), Pipeline(memory=None,\n",
       "           steps=[('inputer',\n",
       "                   SimpleImputer(add_indicator=False, copy=True, fill_value=None,\n",
       "                                 missing_values=nan, strategy='median',\n",
       "                                 verbose=0)),\n",
       "                  ('scaler', MinMaxScaler(copy=True, feature_range=(0, 1))),\n",
       "                  ('est',\n",
       "                   LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
       "                                      fit_intercept=True, intercept_scaling=1,\n",
       "                                      l1_ratio=None, max_iter=100,\n",
       "                                      multi_class='warn', n_jobs=None,\n",
       "                                      penalty='l2', random_state=None,\n",
       "                                      solver='liblinear', tol=0.0001, verbose=0,\n",
       "                                      warm_start=False))],\n",
       "           verbose=False), Pipeline(memory=None,\n",
       "           steps=[('inputer',\n",
       "                   SimpleImputer(add_indicator=False, copy=True, fill_value=None,\n",
       "                                 missing_values=nan, strategy='median',\n",
       "                                 verbose=0)),\n",
       "                  ('scaler', MinMaxScaler(copy=True, feature_range=(0, 1))),\n",
       "                  ('est',\n",
       "                   LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
       "                                      fit_intercept=True, intercept_scaling=1,\n",
       "                                      l1_ratio=None, max_iter=100,\n",
       "                                      multi_class='warn', n_jobs=None,\n",
       "                                      penalty='l2', random_state=None,\n",
       "                                      solver='liblinear', tol=0.0001, verbose=0,\n",
       "                                      warm_start=False))],\n",
       "           verbose=False), Pipeline(memory=None,\n",
       "           steps=[('inputer',\n",
       "                   SimpleImputer(add_indicator=False, copy=True, fill_value=None,\n",
       "                                 missing_values=nan, strategy='median',\n",
       "                                 verbose=0)),\n",
       "                  ('scaler', MinMaxScaler(copy=True, feature_range=(0, 1))),\n",
       "                  ('est',\n",
       "                   LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
       "                                      fit_intercept=True, intercept_scaling=1,\n",
       "                                      l1_ratio=None, max_iter=100,\n",
       "                                      multi_class='warn', n_jobs=None,\n",
       "                                      penalty='l2', random_state=None,\n",
       "                                      solver='liblinear', tol=0.0001, verbose=0,\n",
       "                                      warm_start=False))],\n",
       "           verbose=False), Pipeline(memory=None,\n",
       "           steps=[('inputer',\n",
       "                   SimpleImputer(add_indicator=False, copy=True, fill_value=None,\n",
       "                                 missing_values=nan, strategy='median',\n",
       "                                 verbose=0)),\n",
       "                  ('scaler', MinMaxScaler(copy=True, feature_range=(0, 1))),\n",
       "                  ('est',\n",
       "                   LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
       "                                      fit_intercept=True, intercept_scaling=1,\n",
       "                                      l1_ratio=None, max_iter=100,\n",
       "                                      multi_class='warn', n_jobs=None,\n",
       "                                      penalty='l2', random_state=None,\n",
       "                                      solver='liblinear', tol=0.0001, verbose=0,\n",
       "                                      warm_start=False))],\n",
       "           verbose=False), Pipeline(memory=None,\n",
       "           steps=[('inputer',\n",
       "                   SimpleImputer(add_indicator=False, copy=True, fill_value=None,\n",
       "                                 missing_values=nan, strategy='median',\n",
       "                                 verbose=0)),\n",
       "                  ('scaler', MinMaxScaler(copy=True, feature_range=(0, 1))),\n",
       "                  ('est',\n",
       "                   LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
       "                                      fit_intercept=True, intercept_scaling=1,\n",
       "                                      l1_ratio=None, max_iter=100,\n",
       "                                      multi_class='warn', n_jobs=None,\n",
       "                                      penalty='l2', random_state=None,\n",
       "                                      solver='liblinear', tol=0.0001, verbose=0,\n",
       "                                      warm_start=False))],\n",
       "           verbose=False)),\n",
       " 'test_score': array([0.75045156, 0.71397369, 0.72965704, 0.71505722, 0.73573423,\n",
       "        0.75704081, 0.74149022, 0.75445204, 0.77634985, 0.75830443]),\n",
       " 'train_score': array([0.7804598 , 0.78346922, 0.7825583 , 0.78312935, 0.78205837,\n",
       "        0.77961862, 0.7816293 , 0.77945913, 0.77832441, 0.7797194 ])}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_rez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Алгоритм классификации</th>\n",
       "      <th>ROC-AUC train</th>\n",
       "      <th>ROC-AUC test</th>\n",
       "      <th>Количество фолдов</th>\n",
       "      <th>Время обучения фолда</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.7678</td>\n",
       "      <td>0.7393</td>\n",
       "      <td>10</td>\n",
       "      <td>8.2700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>GradientBoostingClassifier</td>\n",
       "      <td>0.8132</td>\n",
       "      <td>0.7521</td>\n",
       "      <td>10</td>\n",
       "      <td>201.0516</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Алгоритм классификации  ROC-AUC train  ROC-AUC test  Количество фолдов  \\\n",
       "0          LogisticRegression         0.7678        0.7393                 10   \n",
       "1  GradientBoostingClassifier         0.8132        0.7521                 10   \n",
       "\n",
       "   Время обучения фолда  \n",
       "0                8.2700  \n",
       "1              201.0516  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rez_df = pd.DataFrame(rez, columns=['Алгоритм классификации', \n",
    "                                   'ROC-AUC train', 'ROC-AUC test', \n",
    "                                   'Количество фолдов', 'Время обучения фолда'])\n",
    "rez_df = rez_df.round(decimals=4).sort_values(by=['ROC-AUC test'])\n",
    "rez_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Предсказание"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Var1</th>\n",
       "      <th>Var2</th>\n",
       "      <th>Var3</th>\n",
       "      <th>Var4</th>\n",
       "      <th>Var5</th>\n",
       "      <th>Var6</th>\n",
       "      <th>Var7</th>\n",
       "      <th>Var8</th>\n",
       "      <th>Var9</th>\n",
       "      <th>Var10</th>\n",
       "      <th>...</th>\n",
       "      <th>Var221</th>\n",
       "      <th>Var222</th>\n",
       "      <th>Var223</th>\n",
       "      <th>Var224</th>\n",
       "      <th>Var225</th>\n",
       "      <th>Var226</th>\n",
       "      <th>Var227</th>\n",
       "      <th>Var228</th>\n",
       "      <th>Var229</th>\n",
       "      <th>Var230</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1225.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>zCkv</td>\n",
       "      <td>APgdzOv</td>\n",
       "      <td>jySVZNlOJy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ELof</td>\n",
       "      <td>xb3V</td>\n",
       "      <td>6fzt</td>\n",
       "      <td>Zy3gnGM</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>896.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>oslk</td>\n",
       "      <td>IIvC99a</td>\n",
       "      <td>LM8l689qOp</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>xb3V</td>\n",
       "      <td>RAYp</td>\n",
       "      <td>F2FyR07IdsN7I</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>791.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>oslk</td>\n",
       "      <td>6YSocsg</td>\n",
       "      <td>LM8l689qOp</td>\n",
       "      <td>NaN</td>\n",
       "      <td>kG3k</td>\n",
       "      <td>rgKb</td>\n",
       "      <td>RAYp</td>\n",
       "      <td>F2FyR07IdsN7I</td>\n",
       "      <td>mj86</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2296.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>oslk</td>\n",
       "      <td>5nQ7A2G</td>\n",
       "      <td>jySVZNlOJy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>kG3k</td>\n",
       "      <td>rgKb</td>\n",
       "      <td>RAYp</td>\n",
       "      <td>F2FyR07IdsN7I</td>\n",
       "      <td>am7c</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>28.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>oslk</td>\n",
       "      <td>MI8s5nE</td>\n",
       "      <td>LM8l689qOp</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7P5s</td>\n",
       "      <td>RAYp</td>\n",
       "      <td>F2FyR07IdsN7I</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 230 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Var1  Var2  Var3  Var4  Var5    Var6  Var7  Var8  Var9  Var10  ...  \\\n",
       "ID                                                                 ...   \n",
       "0    NaN   NaN   NaN   NaN   NaN  1225.0   7.0   NaN   NaN    NaN  ...   \n",
       "1    NaN   NaN   NaN   NaN   NaN   896.0  14.0   NaN   NaN    NaN  ...   \n",
       "2    NaN   NaN   NaN   NaN   NaN   791.0   7.0   NaN   NaN    NaN  ...   \n",
       "3    NaN   NaN   NaN   NaN   NaN  2296.0   7.0   NaN   NaN    NaN  ...   \n",
       "4    8.0   NaN   NaN   NaN   NaN     NaN   NaN   NaN  28.0    NaN  ...   \n",
       "\n",
       "    Var221   Var222      Var223  Var224  Var225  Var226  Var227  \\\n",
       "ID                                                                \n",
       "0     zCkv  APgdzOv  jySVZNlOJy     NaN    ELof    xb3V    6fzt   \n",
       "1     oslk  IIvC99a  LM8l689qOp     NaN     NaN    xb3V    RAYp   \n",
       "2     oslk  6YSocsg  LM8l689qOp     NaN    kG3k    rgKb    RAYp   \n",
       "3     oslk  5nQ7A2G  jySVZNlOJy     NaN    kG3k    rgKb    RAYp   \n",
       "4     oslk  MI8s5nE  LM8l689qOp     NaN     NaN    7P5s    RAYp   \n",
       "\n",
       "           Var228  Var229  Var230  \n",
       "ID                                 \n",
       "0         Zy3gnGM     NaN     NaN  \n",
       "1   F2FyR07IdsN7I     NaN     NaN  \n",
       "2   F2FyR07IdsN7I    mj86     NaN  \n",
       "3   F2FyR07IdsN7I    am7c     NaN  \n",
       "4   F2FyR07IdsN7I     NaN     NaN  \n",
       "\n",
       "[5 rows x 230 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_kaggle = pd.read_csv('orange_small_churn_test_data.csv', index_col='ID')\n",
    "df_kaggle.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Var205_09_Q</th>\n",
       "      <th>Var205_VpdQ</th>\n",
       "      <th>Var205_other</th>\n",
       "      <th>Var205_sJzTlal</th>\n",
       "      <th>Var212_4kVnq_T26xq1p</th>\n",
       "      <th>Var212_CrNX</th>\n",
       "      <th>Var212_Ie_5MZs</th>\n",
       "      <th>Var212_NhsEn4L</th>\n",
       "      <th>Var212_TW8UhnX</th>\n",
       "      <th>Var212_XfqtO3UdzaXh_</th>\n",
       "      <th>...</th>\n",
       "      <th>Var196_1K8T</th>\n",
       "      <th>Var196_other</th>\n",
       "      <th>Var196_z3mO</th>\n",
       "      <th>Var213_KdSa</th>\n",
       "      <th>Var213_other</th>\n",
       "      <th>Var203_9_Y1</th>\n",
       "      <th>Var203_F3hy</th>\n",
       "      <th>Var203_other</th>\n",
       "      <th>Var191_other</th>\n",
       "      <th>Var191_r__I</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 748 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Var205_09_Q  Var205_VpdQ  Var205_other  Var205_sJzTlal  \\\n",
       "0          NaN          NaN           NaN             NaN   \n",
       "1          NaN          NaN           NaN             NaN   \n",
       "2          NaN          NaN           NaN             NaN   \n",
       "3          NaN          NaN           NaN             NaN   \n",
       "4          NaN          NaN           NaN             NaN   \n",
       "\n",
       "   Var212_4kVnq_T26xq1p  Var212_CrNX  Var212_Ie_5MZs  Var212_NhsEn4L  \\\n",
       "0                   NaN          NaN             NaN             NaN   \n",
       "1                   NaN          NaN             NaN             NaN   \n",
       "2                   NaN          NaN             NaN             NaN   \n",
       "3                   NaN          NaN             NaN             NaN   \n",
       "4                   NaN          NaN             NaN             NaN   \n",
       "\n",
       "   Var212_TW8UhnX  Var212_XfqtO3UdzaXh_  ...  Var196_1K8T  Var196_other  \\\n",
       "0             NaN                   NaN  ...          NaN           NaN   \n",
       "1             NaN                   NaN  ...          NaN           NaN   \n",
       "2             NaN                   NaN  ...          NaN           NaN   \n",
       "3             NaN                   NaN  ...          NaN           NaN   \n",
       "4             NaN                   NaN  ...          NaN           NaN   \n",
       "\n",
       "   Var196_z3mO  Var213_KdSa  Var213_other  Var203_9_Y1  Var203_F3hy  \\\n",
       "0          NaN          NaN           NaN          NaN          NaN   \n",
       "1          NaN          NaN           NaN          NaN          NaN   \n",
       "2          NaN          NaN           NaN          NaN          NaN   \n",
       "3          NaN          NaN           NaN          NaN          NaN   \n",
       "4          NaN          NaN           NaN          NaN          NaN   \n",
       "\n",
       "   Var203_other  Var191_other  Var191_r__I  \n",
       "0           NaN           NaN          NaN  \n",
       "1           NaN           NaN          NaN  \n",
       "2           NaN           NaN          NaN  \n",
       "3           NaN           NaN          NaN  \n",
       "4           NaN           NaN          NaN  \n",
       "\n",
       "[5 rows x 748 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Для получения необходимой размерности, создадим пустую матрицу, с количеством колонок\n",
    "# равным количеству колонок в тренировочной выборке\n",
    "df_test_kagle = pd.DataFrame(np.full((df_kaggle.shape[0], len(df_cat_var_dummies.columns)), np.nan), \n",
    "                  columns=df_cat_var_dummies.columns)\n",
    "df_test_kagle.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 17.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Отберем столбцы, которые были в тренинговом наборе\n",
    "df_cat_var = df_kaggle[df['Var'].unique()].copy()\n",
    "# Переделать, с учетом того, что должны быть и столбцы, которых в тестовом наборе и не получим\n",
    "# учесть и порядок\n",
    "for col in df_cat_var.columns:\n",
    "    val_set = set(df[ df['Var'] == col]['Val'].values)\n",
    "    df_cat_var[col] = df_cat_var[col].apply(lambda x: x if x in val_set else 'other')\n",
    "\n",
    "df_cat_var_dummies_kaggle = pd.get_dummies(df_cat_var)\n",
    "# В этот момент набор df_cat_var_dummies_kaggle содержит не больше столбцов, чем тренинговый набор,\n",
    "# но может содержать меньше (это наиболее вероятно, так как каких-то значений, которые\n",
    "# были в тренинговом наборе, в тестовом может не быть)\n",
    "for col in df_cat_var_dummies_kaggle.columns:\n",
    "    df_test_kagle[col] = df_cat_var_dummies_kaggle[col]\n",
    "    \n",
    "\n",
    "\n",
    "df_test_kaggle_conc = pd.concat([df_kaggle[cat_num], df_test_kagle ], axis=1)\n",
    "X_kaggle = df_test_kaggle_conc.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 922)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_kaggle.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4.54 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('inputer',\n",
       "                 SimpleImputer(add_indicator=False, copy=True, fill_value=None,\n",
       "                               missing_values=nan, strategy='median',\n",
       "                               verbose=0)),\n",
       "                ('scaler', MinMaxScaler(copy=True, feature_range=(0, 1))),\n",
       "                ('est',\n",
       "                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
       "                                    fit_intercept=True, intercept_scaling=1,\n",
       "                                    l1_ratio=None, max_iter=100,\n",
       "                                    multi_class='warn', n_jobs=None,\n",
       "                                    penalty='l2', random_state=None,\n",
       "                                    solver='liblinear', tol=0.0001, verbose=0,\n",
       "                                    warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "si = SimpleImputer(strategy='median')\n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "est = LogisticRegression( solver='liblinear')\n",
    "# est = GradientBoostingClassifier()\n",
    "pipe = Pipeline([\n",
    "    ('inputer', si), \n",
    "    ('scaler', scaler), \n",
    "    ('est', est)\n",
    "])\n",
    "\n",
    "pipe.fit(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_kaggle = pipe.predict_proba(X_kaggle)\n",
    "\n",
    "df_kaggle['result'] = y_kaggle[:,1]\n",
    "df_kaggle[['result']].round(2).to_csv('dminik6_part2_w3_09.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dminik6_part2_w3_05.csv на Kaggle дал 0.70036.\n",
    "е\n",
    "Данные для обучения:  менялись только строковые переменные. После dummies отбрасывались \n",
    "те, у которых коэффициент корреляции с целевой переменной был меньше 0.02 и чье количество было больше 3, такие переменные заменены на 'other'. Отобрано 249 переменных.\n",
    "\n",
    "\n",
    "Обучение на pipe: sSimpleImputer(strategy='median'), preprocessing.MinMaxScaler(), GradientBoostingClassifier(). На тренинговом наборе получено 0.7518 (10 фолдов)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## threshold = 30\n",
    "\n",
    "0.6801 0.738 LogisticRegression  \n",
    "0.6736 0.7365 RidgeClassifier\n",
    "\n",
    "Без preprocessing.MinMaxScaler()  \n",
    "0.5962 0.6077 LogisticRegression  \n",
    "0.6726 0.7372 RidgeClassifier  \n",
    "Wall time: 7min 22s\n",
    "\n",
    "\n",
    "scaler = preprocessing.StandardScaler()  \n",
    "0.6685 0.7465 LogisticRegression  \n",
    "0.6697 0.7376 RidgeClassifier  \n",
    "Wall time: 10min 41s\n",
    "\n",
    "\n",
    "si = SimpleImputer(strategy='median')  \n",
    "0.6791 0.7391 RidgeClassifier  \n",
    "Wall time: 5min 18s\n",
    "\n",
    "\n",
    "si = SimpleImputer(strategy='median')  \n",
    "0.6833 0.7411 LogisticRegression  \n",
    "Wall time: 5min\n",
    "\n",
    "\n",
    "## threshold = 50\n",
    "si = SimpleImputer(strategy='median')  \n",
    "0.684 0.7549 LogisticRegression  \n",
    "0.6798 0.7537 RidgeClassifier  \n",
    "Wall time: 17min 5s  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5\\. Все ли признаки оказались полезными для построения моделей? Проведите процедуру отбора признаков, попробуйте разные варианты отбора (обратите внимание на модуль `sklearn.feature_selection`). Например, можно выбрасывать случайные признаки или строить отбор на основе l1-регуляризации - отфильтровать из обучения признаки, которые получат нулевой вес при построении регрессии с l1-регуляризацией (`sklearn.linear_model.Lasso`). И всегда можно придумать что-то своё=) Попробуйте как минимум 2 различные стратегии, сравните результаты. Помог ли отбор признаков улучшить качество модели? Поясните свой ответ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6\\. Подберите оптимальные параметры модели. Обратите внимание, что в зависимости от того, как вы обработали исходные данные, сделали ли балансировку классов, сколько объектов оставили в обучающей выборке и др. оптимальные значения параметров могут меняться. Возьмите наилучшее из ваших решений на текущий момент и проведите процедуру подбора параметров модели (обратите внимание на `sklearn.model_selection.GridSearchCV`) Как подбор параметров повлиял на качество модели?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7\\. Предложите методику оценки того, какие признаки внесли наибольший вклад в модель (например, это могут быть веса в случае регрессии, а также большое количество моделей реализуют метод `feature_importances_` - оценка важности признаков). На основе предложенной методики проанализируйте, какие признаки внесли больший вклад в модель, а какие меньший?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8\\. Напоследок давайте посмотрим на объекты. На каких объектах достигается наибольшая ошибка классификации? Есть ли межу этими объектами что-то общее? Видны ли какие-либо закономерности? Предположите, почему наибольшая ошибка достигается именно на этих объектах. В данном случае \"наибольшую\" ошибку можно понимать как отнесение объекта с чужому классу с большой долей уверенности (с высокой вероятностью)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9\\. По итогам проведенных экспериментов постройте финальную решение - модель с наилучшим качеством. Укажите, какие преобразования данных, параметры и пр. вы выбрали для построения финальной модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10\\. Подумайте, можно ли еще улучшить модель? Что для этого можно сделать? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
