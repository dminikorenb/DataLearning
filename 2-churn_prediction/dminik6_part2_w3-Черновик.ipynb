{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_validate, cross_val_predict\n",
    "\n",
    "from sklearn.linear_model import RidgeClassifier, LogisticRegression, SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, RepeatedKFold\n",
    "\n",
    "from scipy.stats import pointbiserialr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Var1</th>\n",
       "      <th>Var2</th>\n",
       "      <th>Var3</th>\n",
       "      <th>Var4</th>\n",
       "      <th>Var5</th>\n",
       "      <th>Var6</th>\n",
       "      <th>Var7</th>\n",
       "      <th>Var8</th>\n",
       "      <th>Var9</th>\n",
       "      <th>Var10</th>\n",
       "      <th>...</th>\n",
       "      <th>Var222</th>\n",
       "      <th>Var223</th>\n",
       "      <th>Var224</th>\n",
       "      <th>Var225</th>\n",
       "      <th>Var226</th>\n",
       "      <th>Var227</th>\n",
       "      <th>Var228</th>\n",
       "      <th>Var229</th>\n",
       "      <th>Var230</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3052.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>vr93T2a</td>\n",
       "      <td>LM8l689qOp</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fKCe</td>\n",
       "      <td>02N6s8f</td>\n",
       "      <td>xwM2aC7IdeMC0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1813.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>6hQ9lNX</td>\n",
       "      <td>LM8l689qOp</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ELof</td>\n",
       "      <td>xb3V</td>\n",
       "      <td>RAYp</td>\n",
       "      <td>55YFVY9</td>\n",
       "      <td>mj86</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1953.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>catzS2D</td>\n",
       "      <td>LM8l689qOp</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>FSa2</td>\n",
       "      <td>ZI9m</td>\n",
       "      <td>ib5G6X1eUxUn6</td>\n",
       "      <td>mj86</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1533.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>e4lqvY0</td>\n",
       "      <td>LM8l689qOp</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>xb3V</td>\n",
       "      <td>RAYp</td>\n",
       "      <td>F2FyR07IdsN7I</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>686.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>MAz3HNj</td>\n",
       "      <td>LM8l689qOp</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>WqMG</td>\n",
       "      <td>RAYp</td>\n",
       "      <td>F2FyR07IdsN7I</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 231 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Var1  Var2  Var3  Var4  Var5    Var6  Var7  Var8  Var9  Var10  ...  \\\n",
       "ID                                                                 ...   \n",
       "0    NaN   NaN   NaN   NaN   NaN  3052.0   NaN   NaN   NaN    NaN  ...   \n",
       "1    NaN   NaN   NaN   NaN   NaN  1813.0   7.0   NaN   NaN    NaN  ...   \n",
       "2    NaN   NaN   NaN   NaN   NaN  1953.0   7.0   NaN   NaN    NaN  ...   \n",
       "3    NaN   NaN   NaN   NaN   NaN  1533.0   7.0   NaN   NaN    NaN  ...   \n",
       "4    NaN   NaN   NaN   NaN   NaN   686.0   7.0   NaN   NaN    NaN  ...   \n",
       "\n",
       "     Var222      Var223  Var224  Var225  Var226   Var227         Var228  \\\n",
       "ID                                                                        \n",
       "0   vr93T2a  LM8l689qOp     NaN     NaN    fKCe  02N6s8f  xwM2aC7IdeMC0   \n",
       "1   6hQ9lNX  LM8l689qOp     NaN    ELof    xb3V     RAYp        55YFVY9   \n",
       "2   catzS2D  LM8l689qOp     NaN     NaN    FSa2     ZI9m  ib5G6X1eUxUn6   \n",
       "3   e4lqvY0  LM8l689qOp     NaN     NaN    xb3V     RAYp  F2FyR07IdsN7I   \n",
       "4   MAz3HNj  LM8l689qOp     NaN     NaN    WqMG     RAYp  F2FyR07IdsN7I   \n",
       "\n",
       "    Var229  Var230  Label  \n",
       "ID                         \n",
       "0      NaN     NaN   -1.0  \n",
       "1     mj86     NaN   -1.0  \n",
       "2     mj86     NaN   -1.0  \n",
       "3      NaN     NaN    1.0  \n",
       "4      NaN     NaN   -1.0  \n",
       "\n",
       "[5 rows x 231 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_data = pd.read_csv('not_hold_out_dataset.csv')\n",
    "df_data = pd.read_csv('orange_small_churn_train_data.csv', index_col='ID')\n",
    "df_data.rename(columns={\"labels\": \"Label\"}, inplace=True)\n",
    "df_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = df_data[df_data['Label'].notnull()]\n",
    "\n",
    "df_data['Label'].replace(to_replace=-1, value=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Список столбцов, в котором все строки Nan\n",
    "columns_nan = [ c for c in df_data.columns if df_data[c].isnull().all()  ]\n",
    "# Удаление столбцов\n",
    "df_data = df_data.drop(columns=columns_nan)\n",
    "\n",
    "# columns_nan\n",
    "columns_categ = df_data.select_dtypes(['object']).columns\n",
    "columns_val = set(df_data.columns).difference(set(list(columns_categ) + ['Label', 'ID']))\n",
    "\n",
    "# Список столбцов, в котором все значения равны. НО надо вставить проверку на то,\n",
    "# что это не искомый флаг. Будем удалять столбец, если отношение вероятностей \n",
    "# оттока и не оттока равны.\n",
    "# Наверное, отложим так как лучше в дальнейшем смотртеть через корреляцию.\n",
    "# columns_const = [c for c in df_data.columns if df_data[c].nunique() == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Var118 и Var67 содержат одно значение - 3.0 и 0.0 соответвтенно.\n",
    "# так как при определении коэффициента корреляции черещ pointbiserialr не допустимо\n",
    "# иметь одно значение, заменим в этих переменных Nan на -1.\n",
    "# df_data[['Var118', 'Var67']].max()\n",
    "# df_data[['Var118', 'Var67']].fillna(value=-1, inplace=True)\n",
    "df_data['Var67'].fillna(value=-1, inplace=True)\n",
    "df_data['Var118'].fillna(value=-1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_data['Var191'].nunique()\n",
    "# columns_const\n",
    "# np.asarray([0, 0, 0, 1, 1], dtype = bool)\n",
    "# columns_val\n",
    "# df_data['Var118'].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Var144', 'Var72', 'Var113', 'Var132', 'Var65', 'Var140', 'Var6',\n",
       "       'Var81', 'Var13', 'Var119', 'Var126', 'Var74', 'Var73', 'Var143',\n",
       "       'Var111', 'Var160', 'Var168', 'Var7', 'Var177', 'Var10', 'Var19',\n",
       "       'Var188', 'Var184', 'Var53', 'Var131', 'Var189', 'Var67', 'Var139'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pvalue_border = 0.05\n",
    "acorr = []\n",
    "for c in columns_val:\n",
    "    ser = df_data[['Label',c]].dropna()\n",
    "    rez = pointbiserialr(ser['Label'], ser[c])\n",
    "    acorr.append([c, rez.correlation, rez.pvalue])\n",
    "df_corr_var = pd.DataFrame(acorr, columns=['Var', 'correlation', 'pvalue'], index=None)\n",
    "df_corr_var['correlation_abs'] = df_corr_var['correlation'].abs()\n",
    "# Получен список численных переменных, коррелированных с выходным параметром\n",
    "\n",
    "df_corr_var[df_corr_var['pvalue'] <= pvalue_border].Var.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 129 entries, 0 to 171\n",
      "Data columns (total 4 columns):\n",
      "Var                129 non-null object\n",
      "correlation        129 non-null float64\n",
      "pvalue             129 non-null float64\n",
      "correlation_abs    129 non-null float64\n",
      "dtypes: float64(3), object(1)\n",
      "memory usage: 5.0+ KB\n"
     ]
    }
   ],
   "source": [
    "df_corr_var[df_corr_var['correlation_abs'] > 0.01 ].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns_val_used = df_corr_var[df_corr_var['pvalue'] <= pvalue_border].Var.values\n",
    "columns_val_used = df_corr_var[df_corr_var['correlation_abs'] > 0.01].Var.values\n",
    "\n",
    "columns_val_not_used = set(columns_val).difference(set(columns_val_used))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_data[columns_categ].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 16.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# так как результат предсказания низкий. Пробуем следующее:\n",
    "# в категориальных переменных, обучающей выборки оставим только те значения,\n",
    "# которые есть в тестовой выборке.\n",
    "df_test = pd.read_csv('orange_small_churn_test_data.csv')\n",
    "for c in columns_categ:\n",
    "    enable_val = df_test[c].unique()\n",
    "    df_data[c] = df_data[c].apply(lambda x: x if x in enable_val else np.NaN )\n",
    "df_test = None   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # df_data[columns_categ].info()\n",
    "# df_test = pd.read_csv('orange_small_churn_test_data.csv')\n",
    "# print(df_test['Var215'].unique())\n",
    "# print(df_data['Var215'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 151 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Вычисление корреляции каждой переменной в каждом категориальном параметре\n",
    "# Цель - в дальнейшем оставить переменные и значения с наибольшей корреляцией\n",
    "# или коррелирующиеся.\n",
    "# Так как расчет выполняется несколько минут (только что - 35 сек.), то данные сохранены в файл\n",
    "# acorr = []\n",
    "# for c in columns_categ:\n",
    "#     df_dummies=pd.get_dummies(df_data[c])\n",
    "#     for c_val in df_dummies.columns:\n",
    "#         rez = pointbiserialr(df_data['Label'], df_dummies[c_val])\n",
    "#         acorr.append([c, c_val, rez.correlation, rez.pvalue])\n",
    "# # df_data['Var193']\n",
    "# df_corr_categ = pd.DataFrame(acorr, columns=['Var', 'Val', 'correlation', 'pvalue'], index=None)\n",
    "# df_corr_categ.to_pickle('df_corr_categ.bin')\n",
    "df_corr_categ = pd.read_pickle('df_corr_categ.bin')\n",
    "df_corr_categ['correlation_abs'] = df_corr_categ['correlation'].abs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 606 entries, 27 to 15636\n",
      "Data columns (total 5 columns):\n",
      "Var                606 non-null object\n",
      "Val                606 non-null object\n",
      "correlation        606 non-null float64\n",
      "pvalue             606 non-null float64\n",
      "correlation_abs    606 non-null float64\n",
      "dtypes: float64(3), object(2)\n",
      "memory usage: 28.4+ KB\n"
     ]
    }
   ],
   "source": [
    "df_corr_categ[df_corr_categ['correlation_abs'] >= 0.02].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Var191',\n",
       " 'Var194',\n",
       " 'Var196',\n",
       " 'Var203',\n",
       " 'Var208',\n",
       " 'Var213',\n",
       " 'Var215',\n",
       " 'Var219',\n",
       " 'Var223',\n",
       " 'Var224'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# список категориальных полей, которые останутся, и которые должны быть удалены,\n",
    "# так как их нет в df_corr_categ\n",
    "# ser_corr_categ = df_corr_categ[df_corr_categ['pvalue'] <= pvalue_border] \n",
    "ser_corr_categ = df_corr_categ[df_corr_categ['correlation_abs'] >= 0.02] \n",
    "\n",
    "columns_categ_used = ser_corr_categ['Var'].unique()\n",
    "columns_categ_not_used = set(columns_categ).difference(set(columns_categ_used))\n",
    "columns_categ_not_used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "606\n"
     ]
    }
   ],
   "source": [
    "# ограничение на количество столбцов для анализа. Без этого порядка 2 тыс, идет своп\n",
    "ser_corr_categ = ser_corr_categ.sort_values(by=['correlation_abs']).head(1000)\n",
    "print(len(ser_corr_categ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создание нового массива из необходимых для анализа полей (Можно пойти и наоборот - удалить\n",
    "# поля, которые не нужны).\n",
    "### columns_used = list(columns_val_used) + list(columns_categ_used) + ['Label']\n",
    "df_categ = df_data[columns_categ_used].copy()\n",
    "# df_categ.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# теперь в категориальных полях оставим только коррелирующие значения, заменив остальные на \n",
    "for c in df_categ.columns:\n",
    "    lvals = ser_corr_categ.loc[ser_corr_categ.Var == c].Val.values\n",
    "    df_categ[c] = df_categ[c].apply(lambda x: x if x in lvals else 'Другое')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 18298 entries, 0 to 18297\n",
      "Columns: 633 entries, Var192_4E_Oj1siQ3 to Var229_Другое\n",
      "dtypes: uint8(633)\n",
      "memory usage: 11.2 MB\n"
     ]
    }
   ],
   "source": [
    "df_dummies=pd.get_dummies(df_categ)\n",
    "df_dummies.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.concat([df_data[columns_val_used], df_dummies], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['Var221'].unique()\n",
    "# ser_corr_categ.loc[ser_corr_categ.Var == 'Var221'].Val.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Prepare_dataset():\n",
    "    def __init__(self, border=0.):\n",
    "        # Список столбцов, в котором все строки Nan\n",
    "        self.columns_nan = []\n",
    "        return\n",
    "    \n",
    "    def fit(self, df):\n",
    "        self.columns_nan = [ c for c in df.columns if df[c].isnull().all()  ]\n",
    "\n",
    "        return\n",
    "    \n",
    "    def transform(self, df):\n",
    "        # Удаление столбцов, в которых все строки Nan\n",
    "        dfRet = df.drop(columns=columns_nan)\n",
    "        return dfRet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Удалим строки в которых все значения NaN.\n",
    "# df_tmp = df_data.iloc[:, list(range(0, 190+40)) + [-1]].dropna(axis=1, how='all')\n",
    "# df_tmp.info()\n",
    "print(df_data.info())\n",
    "# df_tmp = df_data.dropna(axis=1, how='all')\n",
    "df_tmp = df_data.drop(columns=columns_nan)\n",
    "\n",
    "print(df_tmp.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обработаем категриальные параметры"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_tmp.dtypes\n",
    "# columns[dtype='object']\n",
    "# df_object = df_tmp.select_dtypes(['object'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Не выполнилось - MemoryError \n",
    "# df_dummies=(pd.get_dummies(df_object) > 0).astype(np.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_dummies.to_csv('df_dummies.csv') - бесконечно долго \n",
    "# to_pickle('df_dummies.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_object = df_object.astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_object.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_object['Var191'].cat.codes\n",
    "# df_object=df_object.appply.cat.codes\n",
    "# df_object.head()\n",
    "# columns = df_object.columns\n",
    "# df_object[columns]=df_object[columns].apply(lambda x: x.cat.codes)\n",
    "# df_object=df_object.apply(lambda x: x.cat.codes)\n",
    "# df_object ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def say_cross_validate_rezult(cv_rez):\n",
    "    print('Алгоритм классификации: {}'.format(type(cv_rez['estimator'][0][1]).__name__))\n",
    "    print('     Количество фолдов: {}'.format(len(cv_rez['estimator'])))\n",
    "\n",
    "    print('  Время обучения фолда: {:.2f}'.format(cv_rez['fit_time'].mean()))\n",
    "    print('               ROC-AUC: {:.4f}'.format(cv_rez['test_roc_auc'].mean()))\n",
    "    print('             precision: {:.4f}'.format(cv_rez['test_precision'].mean()))\n",
    "    print('                recall: {:.4f}'.format(cv_rez['test_recall'].mean()))\n",
    "    print('               Мера f1: {:.4f}'.format(cv_rez['test_f1'].mean()))\n",
    "    print()\n",
    "    \n",
    "def cross_validate_rezult(cv_rez):\n",
    "    return [\n",
    "        type(cv_rez['estimator'][0]).__name__, \n",
    "#         type(cv_rez['estimator'][0][1]).__name__,\n",
    "        cv_rez['test_roc_auc'].mean(),\n",
    "        cv_rez['test_precision'].mean(),\n",
    "        cv_rez['test_recall'].mean(),\n",
    "        cv_rez['test_f1'].mean(),\n",
    "        cv_rez['test_roc_auc'].std(),\n",
    "        cv_rez['test_precision'].std(),\n",
    "        cv_rez['test_recall'].std(),\n",
    "        cv_rez['test_f1'].std(),\n",
    "        len(cv_rez['estimator']),\n",
    "        cv_rez['fit_time'].mean()            \n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сделаем копию данных и заменим столюцы строк (тип object) на их числовые значения (выполним LabelEncoder)\n",
    "# преобразование в лоб всех столбцов по OneHotEncoder без предварительной подготовки\n",
    "# данных приводит к не подъёмному DataFrame\n",
    "df_object = df_tmp.copy()\n",
    "columns = df_object.select_dtypes(['object']).columns\n",
    "df_object[columns] = df_object[columns].astype('category')\n",
    "df_object[columns] = df_object[columns].apply(lambda x: x.cat.codes)\n",
    "df_object.info()\n",
    "# Приведем к виду 0,1 \n",
    "# df_object.loc[df_object['Label'] == -1, 'Label'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = df_test.iloc[:,:-1]\n",
    "# si = SimpleImputer(strategy='most_frequent')\n",
    "si = SimpleImputer(strategy='mean')\n",
    "X = si.fit_transform(df_test)\n",
    "\n",
    "# min_max_scaler = preprocessing.MinMaxScaler()\n",
    "# X = min_max_scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = df_object.iloc[:,:-1]\n",
    "y = df_data['Label']\n",
    "#  most_frequent\n",
    "# pipe = Pipeline([(\"imputer\", SimpleImputer(strategy='most_frequent')),\n",
    "#                       (\"estimator\", RandomForestClassifier(n_estimators=10))])\n",
    "# RidgeClassifier\n",
    "# RandomForestClassifier(n_estimators=10)\n",
    "# GradientBoostingClassifier()\n",
    "# accuracy = cross_val_score(pipe, X, y, cv=3, scoring='accuracy' )\n",
    "\n",
    "# accuracy = cross_val_score(pipe, X, y, cv=3, scoring='f1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "rez = []\n",
    "# cv = StratifiedKFold(n_splits=3, shuffle=True)\n",
    "# rkf = RepeatedKFold(n_splits=10, n_repeats=5)\n",
    "# for est in [KNeighborsClassifier(n_neighbors=5), SGDClassifier(), LogisticRegression(solver='liblinear'), RidgeClassifier(), RandomForestClassifier(n_estimators=10), GradientBoostingClassifier()]:\n",
    "\n",
    "\n",
    "for est in [SGDClassifier(), \n",
    "            LinearSVC(),\n",
    "            LogisticRegression( solver='lbfgs' ),\n",
    "#             LogisticRegression(solver='liblinear'), \n",
    "#             RidgeClassifier(),\n",
    "            RandomForestClassifier(n_estimators=10), \n",
    "            SVC(kernel='linear') \n",
    "#             GradientBoostingClassifier()\n",
    "           ]:\n",
    "\n",
    "# for est in [ RidgeClassifier(), RandomForestClassifier(n_estimators=10)]:\n",
    "#     pipe = Pipeline([(\"imputer\", SimpleImputer(strategy='most_frequent')),\n",
    "#                       (\"estimator\", est)])\n",
    "\n",
    "    cv_rez = cross_validate(est, X, y, cv=3, \n",
    "                          return_train_score=False, \n",
    "                          return_estimator=True,\n",
    "                          verbose=0, n_jobs=4,\n",
    "                          scoring=('accuracy', 'precision', 'recall', 'f1', 'roc_auc'))\n",
    "    rez.append(cross_validate_rezult(cv_rez))\n",
    "#     say_cross_validate_rezult(cv_rez)\n",
    "# cv_rez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Алгоритм классификации</th>\n",
       "      <th>ROC-AUC</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>Мера f1</th>\n",
       "      <th>Дисп. ROC-AUC</th>\n",
       "      <th>Дисп.  precision</th>\n",
       "      <th>Дисп. recall</th>\n",
       "      <th>Дисп. Мера f1</th>\n",
       "      <th>Количество фолдов</th>\n",
       "      <th>Время обучения фолда</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SGDClassifier</td>\n",
       "      <td>0.5571</td>\n",
       "      <td>0.4017</td>\n",
       "      <td>0.0087</td>\n",
       "      <td>0.0165</td>\n",
       "      <td>0.0117</td>\n",
       "      <td>0.1390</td>\n",
       "      <td>0.0064</td>\n",
       "      <td>0.0116</td>\n",
       "      <td>3</td>\n",
       "      <td>3.9721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.5869</td>\n",
       "      <td>0.2778</td>\n",
       "      <td>0.0015</td>\n",
       "      <td>0.0029</td>\n",
       "      <td>0.0085</td>\n",
       "      <td>0.2079</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.0020</td>\n",
       "      <td>3</td>\n",
       "      <td>2.9524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>0.6934</td>\n",
       "      <td>0.6162</td>\n",
       "      <td>0.0886</td>\n",
       "      <td>0.1547</td>\n",
       "      <td>0.0041</td>\n",
       "      <td>0.0306</td>\n",
       "      <td>0.0072</td>\n",
       "      <td>0.0106</td>\n",
       "      <td>3</td>\n",
       "      <td>7.9842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RidgeClassifier</td>\n",
       "      <td>0.7812</td>\n",
       "      <td>0.4023</td>\n",
       "      <td>0.0995</td>\n",
       "      <td>0.1591</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.0147</td>\n",
       "      <td>0.0161</td>\n",
       "      <td>0.0220</td>\n",
       "      <td>3</td>\n",
       "      <td>3.8818</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Алгоритм классификации  ROC-AUC  precision  recall  Мера f1  Дисп. ROC-AUC  \\\n",
       "0           SGDClassifier   0.5571     0.4017  0.0087   0.0165         0.0117   \n",
       "1      LogisticRegression   0.5869     0.2778  0.0015   0.0029         0.0085   \n",
       "3  RandomForestClassifier   0.6934     0.6162  0.0886   0.1547         0.0041   \n",
       "2         RidgeClassifier   0.7812     0.4023  0.0995   0.1591         0.0010   \n",
       "\n",
       "   Дисп.  precision  Дисп. recall  Дисп. Мера f1  Количество фолдов  \\\n",
       "0            0.1390        0.0064         0.0116                  3   \n",
       "1            0.2079        0.0010         0.0020                  3   \n",
       "3            0.0306        0.0072         0.0106                  3   \n",
       "2            0.0147        0.0161         0.0220                  3   \n",
       "\n",
       "   Время обучения фолда  \n",
       "0                3.9721  \n",
       "1                2.9524  \n",
       "3                7.9842  \n",
       "2                3.8818  "
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rez_df = pd.DataFrame(rez, columns=['Алгоритм классификации', \n",
    "                                   'ROC-AUC', 'precision', 'recall', 'Мера f1',\n",
    "                                   'Дисп. ROC-AUC', 'Дисп.  precision', 'Дисп. recall', 'Дисп. Мера f1',\n",
    "                                   'Количество фолдов', 'Время обучения фолда'])\n",
    "rez_df = rez_df.round(decimals=4).sort_values(by=['ROC-AUC'])\n",
    "rez_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ограничение ser_corr_categ 1000  \n",
    "GradientBoostingClassifier - 0.7961 при времени обучения фолда 339.2782 \n",
    "\n",
    "Ограничение ser_corr_categ 800  \n",
    "GradientBoostingClassifier - 0.7899 при времени обучения фолда 221.2789 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Прогнозирование 02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('orange_small_churn_test_data.csv')\n",
    "df_test.rename(columns={\"ID\": \"Id\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([nan,  0.])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test['Var67'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2.26 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_test = pd.read_csv('orange_small_churn_test_data.csv')\n",
    "df_test.rename(columns={\"ID\": \"Id\"}, inplace=True)\n",
    "\n",
    "df_test = df_test.drop(columns=columns_nan)\n",
    "df_test['Var67'].fillna(value=-1, inplace=True)\n",
    "df_test['Var118'].fillna(value=-1, inplace=True)\n",
    "\n",
    "df_categ = df_test[columns_categ_used].copy()\n",
    "# теперь в категориальных полях оставим только коррелирующие значения, заменив остальные на \n",
    "for c in df_categ.columns:\n",
    "    lvals = ser_corr_categ.loc[ser_corr_categ.Var == c].Val.values\n",
    "    df_categ[c] = df_categ[c].apply(lambda x: x if x in lvals else 'Другое')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Columns: 1032 entries, Var192_1JGTmBQZiT to Var229_Другое\n",
      "dtypes: float64(1032)\n",
      "memory usage: 78.7 MB\n"
     ]
    }
   ],
   "source": [
    "# Задача в том, чтобы после get_dummies тестового набора наложить полученные данные на\n",
    "# df_dummies\n",
    "# df_dummies.shape\n",
    "# df_categ.shape\n",
    "empty_arr = np.zeros((df_categ.shape[0], df_dummies.shape[1]))\n",
    "df_dummies_test = pd.DataFrame(empty_arr, columns=df_dummies.columns)\n",
    "df_dummies_test.info()\n",
    "df_dummies_tmp =pd.get_dummies(df_categ)\n",
    "for c in df_dummies_tmp.columns:\n",
    "    if c in df_dummies.columns:\n",
    "        df_dummies_test[c] = df_dummies_tmp[c]\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_X = pd.concat([df_test[columns_val_used], df_dummies_test], axis=1)\n",
    "\n",
    "# si = SimpleImputer(strategy='most_frequent')\n",
    "si = SimpleImputer(strategy='mean')\n",
    "\n",
    "X_test = si.fit_transform(df_test_X)\n",
    "# min_max_scaler = preprocessing.MinMaxScaler()\n",
    "# X_test = min_max_scaler.fit_transform (X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.81 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda2\\envs\\newpy\\lib\\site-packages\\sklearn\\linear_model\\ridge.py:147: LinAlgWarning: Ill-conditioned matrix (rcond=1.97755e-18): result may not be accurate.\n",
      "  overwrite_a=True).T\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# est = GradientBoostingClassifier()\n",
    "est = RidgeClassifier()\n",
    "est.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'RidgeClassifier' object has no attribute 'predict_proba'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-85-e9553b6411c0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdf_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'result'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mdf_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Id'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'result'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'dminik6_part2_w3_02.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'RidgeClassifier' object has no attribute 'predict_proba'"
     ]
    }
   ],
   "source": [
    "y_test = est.predict_proba(X_test)\n",
    "df_test['result'] = y_test[:,1]\n",
    "df_test[['Id', 'result']].round(2).to_csv('dminik6_part2_w3_02.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Прогнозирование 01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('orange_small_churn_test_data.csv')\n",
    "df_test.rename(columns={\"ID\": \"Id\"}, inplace=True)\n",
    "\n",
    "\n",
    "columns = df_test.select_dtypes(['object']).columns\n",
    "df_test[columns] = df_test[columns].astype('category')\n",
    "df_test[columns] = df_test[columns].apply(lambda x: x.cat.codes)\n",
    "df_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = df_test.iloc[:,:-1]\n",
    "si = SimpleImputer(strategy='mean')\n",
    "\n",
    "X_test = si.fit_transform(X_test)\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "X_test = min_max_scaler.fit_transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est = GradientBoostingClassifier()\n",
    "est.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = est.predict_proba(X_test)\n",
    "# y_pred = est.predict(X_test)\n",
    "\n",
    "# y_test[:,1]\n",
    "df_test['result'] = y_test[:,1]\n",
    "df_test[['Id', 'result']].round(2).to_csv('dminik6_part2_w3_00.csv', index=False)\n",
    "# df_test[['Id', 'y']].to_csv('dminik6_part2_w3_00.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rez_df.to_csv('01-С нормализацией X.csv')\n",
    "rez_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "say_cross_validate_rezult(cv_rez)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Алгоритм классификации: RidgeClassifier  \n",
    "     Количество фолдов: 3  \n",
    "  Время обучения фолда: 0.67  \n",
    "               ROC-AUC: 0.6711  \n",
    "             precision: 0.0000  \n",
    "                recall: 0.0000  \n",
    "               Мера f1: 0.0000  \n",
    "\n",
    "Алгоритм классификации: RandomForestClassifier  \n",
    "     Количество фолдов: 3  \n",
    "  Время обучения фолда: 1.51  \n",
    "               ROC-AUC: 0.5717  \n",
    "             precision: 0.1500  \n",
    "                recall: 0.0613  \n",
    "               Мера f1: 0.0421  \n",
    "\n",
    "Алгоритм классификации: GradientBoostingClassifier  \n",
    "     Количество фолдов: 3  \n",
    "  Время обучения фолда: 17.48  \n",
    "               ROC-AUC: 0.7060  \n",
    "             precision: 0.2011  \n",
    "                recall: 0.3094  \n",
    "               Мера f1: 0.0627  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Выбор переменных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = pd.read_csv('orange_small_churn_train_data.csv')\n",
    "df_data.rename(columns={\"labels\": \"Label\"}, inplace=True)\n",
    "# категориальные переменные с наибольшим коэффициентом корреляции\n",
    "cat_var = ['Var199','Var192','Var216', 'Var206','Var212','Var205','Var228','Var193','Var207','Var227', 'Label']\n",
    "df_tmp=df_data[np.isfinite(df_data['Label'])].dropna(axis=1, how='all')\n",
    "df_cat_var = df_tmp[cat_var].copy()\n",
    "# df_cat_var.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df = df_cat_var[['Var192', 'Label']]\n",
    "# Категории, которых нет в положительном результате удалим\n",
    "categories_allowed = {}\n",
    "df_positive = df_cat_var[df_cat_var['Label'] > 0 ].index\n",
    "for col in cat_var:\n",
    "    allowed_vals =  df_cat_var[df_cat_var.index.isin(df_positive)][col].value_counts().index\n",
    "    categories_allowed[col] = \n",
    "    df_cat_var.loc[~df_cat_var[col].isin(allowed_vals), col] = np.nan\n",
    "# df_positive\n",
    "# allowed_vals = df[df_positive].Var192.value_counts().index\n",
    "# # .groupby(['Var192'])\n",
    "\n",
    "# df_cat_var.loc[~df['Var192'].isin(allowed_vals), 'Var192'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_cat_var['Var192'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для категориальных предикторов с большим количеством категорий объединим редкие категории в одну (\"other\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_count = df_cat_var['Var199'].value_counts()\n",
    "# Блок позволяет подобрать порог отсечения мелких категорий (проценты от max к)\n",
    "# исходя из желаемого количества столбцов\n",
    "n_columns = 0.\n",
    "for col in cat_var:\n",
    "    val_count = df_cat_var[col].value_counts()\n",
    "    new_cols = val_count.shape[0]\n",
    "    if new_cols > 100:\n",
    "        threshold = val_count.max() * 0.05\n",
    "        new_cols = val_count[ val_count > threshold ].shape[0] \n",
    "        print('{} {:>4.0f} {:>4.0f}'.format(col, val_count.shape[0], new_cols ))\n",
    "        to_remove = val_count[val_count <= threshold].index\n",
    "        df_cat_var[col].replace(to_remove, 'other', inplace=True)\n",
    "    \n",
    "    n_columns += new_cols \n",
    "# val_count.shape[0]\n",
    "print('Columns:', n_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cat_var.drop('Label', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_cat_var.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_cat_var = df_cat_var.astype('category')\n",
    "# df_cat_var = df_cat_var.apply(lambda x: x.cat.codes)\n",
    "df_cat_var_dummies = pd.get_dummies(df_cat_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_cat_var_dummies.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_tmp.select_dtypes(['object']).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_var = ['Var131','Var69','Var53','Var177','Var180','Var114','Var111','Var153',\n",
    "           'Var154','Var59','Var124','Var10','Var33','Var139','Var38','Var5','Var182','Var36',\n",
    "           'Var92']\n",
    "\n",
    "df_test = pd.concat([df_tmp[num_var].fillna(-1), df_cat_var_dummies, df_tmp['Label']], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_tmp[num_var].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cat_var_dummies = None\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_test.iloc[:,:-1]\n",
    "# si = SimpleImputer(strategy='most_frequent')\n",
    "si = SimpleImputer(strategy='mean')\n",
    "\n",
    "X = si.fit_transform(X)\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "X = min_max_scaler.fit_transform(X)\n",
    "\n",
    "y = df_test.iloc[:,-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[np.isnan(y)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "rez = []\n",
    "cv = StratifiedKFold(n_splits=10, shuffle=True)\n",
    "# for est in [SGDClassifier(), LogisticRegression(solver='liblinear'), RidgeClassifier(), RandomForestClassifier(n_estimators=10), GradientBoostingClassifier()]:\n",
    "\n",
    "for est in [ LogisticRegression(solver='liblinear') ]:\n",
    "#     pipe = Pipeline([(\"imputer\", SimpleImputer(strategy='most_frequent')),\n",
    "#                       (\"estimator\", est)])\n",
    "\n",
    "    cv_rez = cross_validate(est, X, y, cv=cv, \n",
    "                          return_train_score=False, \n",
    "                          return_estimator=True,\n",
    "                          verbose=2, n_jobs=4,\n",
    "                          scoring=('accuracy', 'precision', 'recall', 'f1', 'roc_auc'))\n",
    "    rez.append(cross_validate_rezult(cv_rez))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rez_df = pd.DataFrame(rez, columns=['Алгоритм классификации', \n",
    "                                   'ROC-AUC', 'precision', 'recall', 'Мера f1',\n",
    "                                   'Дисп. ROC-AUC', 'Дисп.  precision', 'Дисп. recall', 'Дисп. Мера f1',\n",
    "                                   'Количество фолдов', 'Время обучения фолда'])\n",
    "rez_df.round(decimals=4).sort_values(by=['ROC-AUC'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Предсказание 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('orange_small_churn_test_data.csv')\n",
    "df_test.rename(columns={\"labels\": \"Label\"}, inplace=True)\n",
    "df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est = LogisticRegression(solver='liblinear')\n",
    "# proba = cross_val_predict(est, X, y, cv=cv, method='predict_proba')\n",
    "proba = cross_val_predict(est, X, y, cv=5, method='predict_proba')\n",
    "\n",
    "proba.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rez_df.round(decimals=4).sort_values(by=['ROC-AUC'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Проведем работы с полями, определенными Boruta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_boruta = pd.read_csv('feature_df_01.csv')\n",
    "boruta_columns = df_boruta[df_boruta['rank'] == 1]['features'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_columns = df_tmp[boruta_columns].select_dtypes(['object']).columns\n",
    "# df_object[columns] = df_object[columns].astype('category')\n",
    "# df_object[columns] = df_object[columns].apply(lambda x: x.cat.codes)\n",
    "# df_object.info()\n",
    "cat_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_columns = list(set(boruta_columns) - set(cat_columns))\n",
    "df_tmp = df_data.dropna(axis=1, how='all').copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_tmp[cat_columns].info()\n",
    "n_columns = 0.\n",
    "for col in cat_columns:\n",
    "    val_count = df_tmp[col].value_counts()\n",
    "    new_cols = val_count.shape[0]\n",
    "    if new_cols > 100:\n",
    "        threshold = 10 # val_count.max() * 0.15\n",
    "        new_cols = val_count[ val_count > threshold ].shape[0] \n",
    "        print('{} {:>4.0f} {:>4.0f}'.format(col, val_count.shape[0], new_cols ))\n",
    "#         to_remove = val_count[val_count <= threshold].index\n",
    "#         df_tmp[col].replace(to_remove, 'other', inplace=True)\n",
    "    \n",
    "    n_columns += new_cols \n",
    "# val_count.shape[0]\n",
    "print('Columns:', n_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Подготовка категориальных переменных методом LabelEncoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cat_var_dummies = df_tmp[cat_columns].astype('category').copy()\n",
    "df_cat_var_dummies = df_cat_var_dummies.apply(lambda x: x.cat.codes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Подготовка категориальных переменных методом OneHotEncoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_cat_var_dummies = pd.get_dummies(df_tmp[cat_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_cat_var_dummies.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.concat([df_tmp[var_columns], df_cat_var_dummies, df_tmp['Label']], axis=1)\n",
    "df_cat_var_dummies = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_test.iloc[:,:-1]\n",
    "# si = SimpleImputer(strategy='most_frequent')\n",
    "si = SimpleImputer(strategy='mean')\n",
    "\n",
    "X = si.fit_transform(X)\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "X = min_max_scaler.fit_transform(X)\n",
    "\n",
    "y = df_test.iloc[:,-1]\n",
    "df_test = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "rez = []\n",
    "cv = StratifiedKFold(n_splits=10, shuffle=True)\n",
    "for est in [SGDClassifier(), LogisticRegression(solver='liblinear'), RidgeClassifier(), RandomForestClassifier(n_estimators=10), GradientBoostingClassifier()]:\n",
    "\n",
    "# for est in [ LogisticRegression(solver='liblinear'), RandomForestClassifier(n_estimators=10), GradientBoostingClassifier()]:\n",
    "#     pipe = Pipeline([(\"imputer\", SimpleImputer(strategy='most_frequent')),\n",
    "#                       (\"estimator\", est)])\n",
    "\n",
    "    cv_rez = cross_validate(est, X, y, cv=cv, \n",
    "                          return_train_score=False, \n",
    "                          return_estimator=True,\n",
    "                          verbose=2, n_jobs=4,\n",
    "                          scoring=('accuracy', 'precision', 'recall', 'f1', 'roc_auc'))\n",
    "    rez.append(cross_validate_rezult(cv_rez))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rez_df = pd.DataFrame(rez, columns=['Алгоритм классификации', \n",
    "                                   'ROC-AUC', 'precision', 'recall', 'Мера f1',\n",
    "                                   'Дисп. ROC-AUC', 'Дисп.  precision', 'Дисп. recall', 'Дисп. Мера f1',\n",
    "                                   'Количество фолдов', 'Время обучения фолда'])\n",
    "rez_df.round(decimals=4).sort_values(by=['ROC-AUC'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:newpy] *",
   "language": "python",
   "name": "conda-env-newpy-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
